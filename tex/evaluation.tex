\section{Pure approach}
\subsection{Overall Performance}
After all the necessary content was created by the crowd, the evaluation was done by comparing the ground truth with the generated information. The evaluation task presented the fields title, description and category to the workers. They had to decide which auction description is the best one in their opinion. The result is illustrated in figure \ref{crowdsourcing_eval}. The commission approach got a majority of the votes with 57.14\%. The table received all five votes from the crowd. The ground truth contains unnecessary information and looks like a scam, is the statement of the participants.\\
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/crowdsourcing/plot_evaluation_all.png}
\caption{Evaluation of Ground Truth vs. Crowdsourcing}
\label{crowdsourcing_eval}
\end{figure}
The workers had the possibility to reason their decision. They motivate their votes as follows: 
\begin{itemize}
	\item Higher value of information 
	\item Professionalism 
	\item Hidden information are given (e.g. size of the cleats, size of the smartphone storage) 
	\item The description is clear, short and to the point 
	\item Authenticity of the article 
	\item Grammatical issues 
\end{itemize}

\subsection{Title}
The length of the title (Figure \ref{crowdsourcing_title_length}) increases if the task contains all available pictures of the item. This setup produced an average title length of 64 characters. The other experiments have an average of 50 for the standard design and 48 for a promised commission. The median value shows almost the same ranking except the additional reward in form of a commission generates more characters than the standard configuration. The item without a brand was described by a title with 37 characters. The length doesn't say anything about the quality of the title, but it indicates the influence of the different task settings. The usage of natural language processing tools could help to get more information about the quality, but this would be outside the scope of the thesis.
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/crowdsourcing/plot_title_length.png}
\caption{Evaluation of Title Lengths}
\label{crowdsourcing_title_length}
\end{figure} 

After the creation of the titles, the workers had to select the final one for each item. They justify their selections as follows:
\begin{itemize}
	\item Amount of details 
	\item Attracts more attention 
	\item Research on eBay produces better results for the title 
	\item Experiences with online auctions 
	\item Wrong information 
	\item Too long 
\end{itemize}

\subsection{Description}
The same measurements as for the title were done for the description (Figure \ref{crowdsourcing_desc_length}). The general task design achieved the highest average length (467 chars), but it contains an outlier with 1706 characters for the ground truth item number 5. The worker copied an item description from the website of the producer. More images doesn't conclude to a longer description, because the average length of these titles is 281. An extra reward leads to 455 characters, but has the highest median of 572. The numeric parameter is more robust to outliers. The other setups follow by 307 (Commission) and 238 (Pure). The non-branded item realise a length of 240 characters which is equal to the median of the branded items. Therefore, the workers are able to write descriptions of equal length regardless of which item type (branded/non-branded) was presented to them.
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/crowdsourcing/plot_description_length.png}
\caption{Evaluation of Description Lengths}
\label{crowdsourcing_desc_length}
\end{figure}

\subsection{Category}
Most of the workers submitted only the main category and not the complete hierarchy with main and sub categories. For the football trading cards they wrote ''Sports Mem, Cards \& Fan Shop'' instead of ''Sports Mem, Cards \& Fan Shop:Cards:Football'', for example. One reason could be that the instructions weren't clear and precise enough. Also the desired format of the input wasn't mentioned. An improved task design will result in more accurate outputs.
\subsection{Price Estimation}
To evaluate the accuracy of the price estimations (Figure \ref{crowdsourcing_price_pred}), the root mean squared error (RMSE) was used. It is frequently used to measure the quality of predictions. The true values are the end prices from the ground truth. If the workers have the actual market price of the item at one's disposal then they predict the price best with an average RMSE of 51.46 USD. The experiment with the commission ranked just behind with 58.55 USD, the others appear with 81.75 (Pure) and 89.89 (Image quantity/quality) at the end of the ranking. The tested item without a brand has a RMSE of 278.62 USD which indicates the challenge of the price prediction for generic things.\\
The estimation errors can be explained by different observations. The size of the smartphone storage isn't given. The price difference of an iPhone 5S with 32GB and 64GB is about 100 dollars. The football trading cards are difficult to estimate, because most of the workers don't know the football players and cannot asses the rarity or popularity of them. The football boot model is available in different versions. The original one and the cheaper replica. The action figure is rare and no other auctions are available to compare with. The women handbag could be a fake or an original Coach\footnote{http://www.coach.com} bag.
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/crowdsourcing/plot_price_rmse.png}
\caption{Price Prediction Quality}
\label{crowdsourcing_price_pred}
\end{figure}
Under/Over estimations
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/crowdsourcing/plot_price_overestimation.png}
\caption{Price Overestimations}
\label{crowdsourcing_price_overestimations}
\end{figure}
\subsection{Variations}
\subsubsection{Commission}
The ground truth items 1 and 3 received a majority of the votes from the crowd. The commissions were paid manually using the web interface of the Amazon Mechanical Turk web service. The value of the bonus has to be shortened to two digits after the point and rounded up to \$0.01, because of some restrictions of MTurk. The bonus aggregates to \$1.48 for both auctions. The worker received also an additional message: \\
''Dear worker, \\

You receive a commission (0.25\% of the end price) as bonus payment for your work. The end price of the eBay online auction was \$27.''
\begin{table}[h!]
	\begin{center}
	\begin{tabular}{| p{1.16cm} | p{2.16cm} | p{2.16cm} | p{1.66cm} | p{2.16cm} | p{3.66cm} |}
		\hline
		Ground truth number & End price (USD) & Task & Percentage & Bonus (USD) & Worker ID \\
		\hline
		1 & 4.99 & Title (Finding) & 0.25\% & 0.01 & A3HE1W5T6QO03X \\
		\hline
		1 & 4.99 & Title (Voting) & 0.1\% & 0.01 & A3N7O1NOBGX6U7 \\
		\hline
		1 & 4.99 & Title (Voting) & 0.1\% & 0.01 & A1DK26QAO4OOMQ \\
		\hline
		1 & 4.99 & Description (Improving) & 1\% & 0.05 & A2Y9ZNZ0F24GHB \\
		\hline
		1 & 4.99 & Description (Voting) & 0.05\% & 0.01 & A2FF8HA1OWKS83 \\
		\hline
		1 & 4.99 & Description (Voting) & 0.05\% & 0.01 & AJAOE1PSNKGUE \\
		\hline
		1 & 4.99 & Category & 0.25\% & 0.01 & A2ZT4MTMEVSLB9 \\
		\hline
		1 & 4.99 & Category & 0.25\% & 0.01 & A220ED0LJITW5I \\
		\hline
		1 & 4.99 & Category & 0.25\% & 0.01 & A2V8WJXA0USMZ \\
		\hline
		1 & 4.99 & Price & 0.5\% & 0.02 & A3L99RGPK6FZGH \\
		\hline
		3 & 27 & Title (Finding) & 0.25\% & 0.06 & A23BCMQN9ZU97B \\
		\hline
		3 & 27 & Title (Voting) & 0.1\% & 0.02 & A3N7O1NOBGX6U7 \\
		\hline
		3 & 27 & Title (Voting) & 0.1\% & 0.02 & A3I4BYP4DUC475 \\
		\hline
		3 & 27 & Description (Improving) & 1\% & 0.27 & A1IA4CST74I1Q8 \\
		\hline
		3 & 27 & Description (Voting) & 0.05\% & 0.01 & A3K77RSYXLLUQL \\
		\hline
		3 & 27 & Description (Voting) & 0.05\% & 0.01 & A25F7BNXEN8I5X \\
		\hline
		3 & 27 & Category & 0.25\% & 0.06 & A2ZT4MTMEVSLB9 \\
		\hline
		3 & 27 & Category & 0.25\% & 0.06 & A220ED0LJITW5I \\
		\hline
		3 & 27 & Category & 0.25\% & 0.06 & A2V8WJXA0USMZ \\
		\hline
		3 & 27 & Price & 0.5\% & 0.13 & A3L99RGPK6FZGH \\
		\hline
	\end{tabular}
	\end{center}
	\caption{Commission results}
\end{table}



 

\section{Hybrid approach}
The Random Forest approach shows the best performance over all three item categories independent of classification or regression (Figure). The accuracy of the classifier depends also on the item category. The RFC assign every third input to the right Sony Playstation price class. The plot of the mean absolute error (MAE, Figure) assured the strength of the classifier and visualise how far away the predictions are in average. kNN has an error of about four classes for the iPhone category, one class has a range of 25 USD. The comparison between the human-based and the machine-based prediction indicates that the crowd can't be as accurate as a machine. The lowest RMSE of the ground truth item number 2 (Apple iPhone) is \$76.32. Two machine learning algorithms have a smaller error (Figure). 
The normalised confusion matrix (Figure) of the iPhone classifiers illustrates the distribution of the assignments. The origin is located at the left top position, the x-axis represents the truth-values and the y-axis the values assigned by the classifier. The values were normalised per row. A perfect classifier would produce a diagonal which contains only ones. 
The scatter plot (Figure) is another way to visualise the relations between the truth and predicted values. The criteria of a perfect classifier are the same as for the confusion matrix. 
The same plots for the other categories and the exact accuracies/RMSEs/MAEs are enumerated in the appendix section (Reference).
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/machine_learning/iphone/true_pred_iphone.png}
\caption{Scatter plot true vs. prediction (iPhone)}
\label{truth_predict_iphone}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/machine_learning/iphone/conf_mat_iphone.png}
\caption{Normalised confusion matrix (iPhone)}
\label{crowdsourcing_desc_length}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/machine_learning/plot_price_classification_acc.png}
\caption{Classification accuracy}
\label{crowdsourcing_desc_length}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/machine_learning/plot_price_classification_mae.png}
\caption{Classification mean absolute error}
\label{crowdsourcing_desc_length}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.55]{images/plots/machine_learning/plot_price_regression_rmse.png}
\caption{Regression root mean squared error}
\label{crowdsourcing_desc_length}
\end{figure}



